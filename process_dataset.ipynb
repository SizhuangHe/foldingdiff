{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh2748/.conda/envs/foldingdiff/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import argparse\n",
    "import functools\n",
    "from datetime import datetime\n",
    "from typing import *\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.strategies.ddp import DDPStrategy\n",
    "\n",
    "from transformers import BertConfig\n",
    "\n",
    "from foldingdiff import datasets\n",
    "from foldingdiff import modelling\n",
    "from foldingdiff import losses\n",
    "from foldingdiff import beta_schedules\n",
    "from foldingdiff import plotting\n",
    "from foldingdiff import utils\n",
    "from foldingdiff import custom_metrics as cm\n",
    "\n",
    "assert torch.cuda.is_available(), \"Requires CUDA to train\"\n",
    "# reproducibility\n",
    "torch.manual_seed(6489)\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define some typing literals\n",
    "ANGLES_DEFINITIONS = Literal[\n",
    "    \"canonical\", \"canonical-full-angles\", \"canonical-minimal-angles\", \"cart-coords\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles_definitions = \"canonical-full-angles\"\n",
    "dataset_key = \"cath\"\n",
    "max_seq_len = 128\n",
    "min_seq_len = 60\n",
    "seq_trim_strategy = \"leftalign\"\n",
    "toy = False\n",
    "train_only = False\n",
    "\n",
    "clean_dset_class = {\n",
    "        \"canonical\": datasets.CathCanonicalAnglesDataset,\n",
    "        \"canonical-full-angles\": datasets.CathCanonicalAnglesOnlyDataset,\n",
    "        \"canonical-minimal-angles\": datasets.CathCanonicalMinimalAnglesDataset,\n",
    "        \"cart-coords\": datasets.CathCanonicalCoordsDataset,\n",
    "}[angles_definitions] # select one clean dataset class\n",
    "logging.info(f\"Clean dataset class: {clean_dset_class}\")\n",
    "\n",
    "splits = [\"train\"] if train_only else [\"train\", \"validation\", \"test\"]\n",
    "logging.info(f\"Creating data splits: {splits}\")\n",
    "clean_dsets = [\n",
    "    clean_dset_class(\n",
    "        pdbs=dataset_key,\n",
    "        split=s,\n",
    "        pad=max_seq_len,\n",
    "        min_length=min_seq_len,\n",
    "        trim_strategy=seq_trim_strategy,\n",
    "        zero_center=False if angles_definitions == \"cart-coords\" else True,\n",
    "        toy=toy,\n",
    "    )\n",
    "    for s in splits\n",
    "]\n",
    "train_dataset, valid_dataset, test_dataset = clean_dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22780, 2847, 2849)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(ds):\n",
    "    data_list = [ds[i] for i in range(len(ds))]\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df['lengths'] = df['lengths'].apply(lambda x: x.item())\n",
    "    df = df.sort_values(by='lengths')\n",
    "    filtered_df = df[(df['lengths'] >= 60) & (df['lengths'] <= 128)]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train_ds = process_dataset(train_dataset)\n",
    "filtered_val_ds = process_dataset(valid_dataset)\n",
    "filtered_test_ds = process_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angles</th>\n",
       "      <th>coords</th>\n",
       "      <th>attn_mask</th>\n",
       "      <th>position_ids</th>\n",
       "      <th>lengths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13945</th>\n",
       "      <td>[[tensor(0.), tensor(2.9529), tensor(0.0445), ...</td>\n",
       "      <td>[[tensor(22.5130), tensor(72.0890), tensor(63....</td>\n",
       "      <td>[tensor(1.), tensor(1.), tensor(1.), tensor(1....</td>\n",
       "      <td>[tensor(0), tensor(1), tensor(2), tensor(3), t...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15539</th>\n",
       "      <td>[[tensor(0.), tensor(2.6073), tensor(0.0185), ...</td>\n",
       "      <td>[[tensor(7.4850), tensor(15.6570), tensor(73.4...</td>\n",
       "      <td>[tensor(1.), tensor(1.), tensor(1.), tensor(1....</td>\n",
       "      <td>[tensor(0), tensor(1), tensor(2), tensor(3), t...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6425</th>\n",
       "      <td>[[tensor(0.), tensor(1.6765), tensor(0.0135), ...</td>\n",
       "      <td>[[tensor(8.8570), tensor(-1.3570), tensor(11.3...</td>\n",
       "      <td>[tensor(1.), tensor(1.), tensor(1.), tensor(1....</td>\n",
       "      <td>[tensor(0), tensor(1), tensor(2), tensor(3), t...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11673</th>\n",
       "      <td>[[tensor(0.), tensor(2.3735), tensor(0.0204), ...</td>\n",
       "      <td>[[tensor(105.3070), tensor(10.6050), tensor(64...</td>\n",
       "      <td>[tensor(1.), tensor(1.), tensor(1.), tensor(1....</td>\n",
       "      <td>[tensor(0), tensor(1), tensor(2), tensor(3), t...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6442</th>\n",
       "      <td>[[tensor(0.), tensor(1.1725), tensor(-0.0099),...</td>\n",
       "      <td>[[tensor(30.4300), tensor(1.1830), tensor(2.79...</td>\n",
       "      <td>[tensor(1.), tensor(1.), tensor(1.), tensor(1....</td>\n",
       "      <td>[tensor(0), tensor(1), tensor(2), tensor(3), t...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  angles  \\\n",
       "13945  [[tensor(0.), tensor(2.9529), tensor(0.0445), ...   \n",
       "15539  [[tensor(0.), tensor(2.6073), tensor(0.0185), ...   \n",
       "6425   [[tensor(0.), tensor(1.6765), tensor(0.0135), ...   \n",
       "11673  [[tensor(0.), tensor(2.3735), tensor(0.0204), ...   \n",
       "6442   [[tensor(0.), tensor(1.1725), tensor(-0.0099),...   \n",
       "\n",
       "                                                  coords  \\\n",
       "13945  [[tensor(22.5130), tensor(72.0890), tensor(63....   \n",
       "15539  [[tensor(7.4850), tensor(15.6570), tensor(73.4...   \n",
       "6425   [[tensor(8.8570), tensor(-1.3570), tensor(11.3...   \n",
       "11673  [[tensor(105.3070), tensor(10.6050), tensor(64...   \n",
       "6442   [[tensor(30.4300), tensor(1.1830), tensor(2.79...   \n",
       "\n",
       "                                               attn_mask  \\\n",
       "13945  [tensor(1.), tensor(1.), tensor(1.), tensor(1....   \n",
       "15539  [tensor(1.), tensor(1.), tensor(1.), tensor(1....   \n",
       "6425   [tensor(1.), tensor(1.), tensor(1.), tensor(1....   \n",
       "11673  [tensor(1.), tensor(1.), tensor(1.), tensor(1....   \n",
       "6442   [tensor(1.), tensor(1.), tensor(1.), tensor(1....   \n",
       "\n",
       "                                            position_ids  lengths  \n",
       "13945  [tensor(0), tensor(1), tensor(2), tensor(3), t...       60  \n",
       "15539  [tensor(0), tensor(1), tensor(2), tensor(3), t...       60  \n",
       "6425   [tensor(0), tensor(1), tensor(2), tensor(3), t...       60  \n",
       "11673  [tensor(0), tensor(1), tensor(2), tensor(3), t...       60  \n",
       "6442   [tensor(0), tensor(1), tensor(2), tensor(3), t...       60  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_train_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(df, split:str):\n",
    "    angles = torch.stack(df['angles'].tolist())\n",
    "    lengths = torch.tensor(df['lengths'].tolist())\n",
    "    torch.save(angles, f\"protein_angles_{split}.pt\")\n",
    "    torch.save(lengths, f\"protein_lengths_{split}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(filtered_train_ds, \"train\")\n",
    "save_dataset(filtered_val_ds, \"val\")\n",
    "save_dataset(filtered_test_ds, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/tmp.RIDayGxO54/ipykernel_2493705/2423798540.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data = torch.load(data_path)\n",
      "/tmp/tmp.RIDayGxO54/ipykernel_2493705/2423798540.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.lengths = torch.load(lengths_path)\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, lengths_path):\n",
    "        self.data = torch.load(data_path)\n",
    "        self.lengths = torch.load(lengths_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.lengths[idx]\n",
    "\n",
    "train_ds = CustomDataset(\"protein_angles_train.pt\", \"protein_lengths_train.pt\")\n",
    "val_ds = CustomDataset(\"protein_angles_val.pt\", \"protein_lengths_val.pt\")\n",
    "test_ds = CustomDataset(\"protein_angles_test.pt\", \"protein_lengths_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 6]), tensor(60))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][0].shape, train_ds[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "class LengthBasedSampler(Sampler):\n",
    "    def __init__(self, lengths, batch_size):\n",
    "        self.lengths = lengths\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Group indices by length\n",
    "        self.indices_by_length = {}\n",
    "        for idx, length in enumerate(lengths):\n",
    "            if length.item() not in self.indices_by_length:\n",
    "                self.indices_by_length[length.item()] = []\n",
    "            self.indices_by_length[length.item()].append(idx)\n",
    "\n",
    "        # Create a list of batches where all sequences in each batch have the same length\n",
    "        self.batches = []\n",
    "        for length, indices in self.indices_by_length.items():\n",
    "            for i in range(0, len(indices), batch_size):\n",
    "                self.batches.append(indices[i:i + batch_size])\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # Shuffle the batches for randomness\n",
    "        return iter(self.batches)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(lengths_path, batch_size, ds, df):\n",
    "    sampler = LengthBasedSampler(torch.load(lengths_path), batch_size)\n",
    "    loader = DataLoader(ds, batch_sampler=sampler)\n",
    "    lengths = {key: len(value) for key, value in sampler.indices_by_length.items()}\n",
    "    original_lengths = df['lengths'].value_counts().to_dict()\n",
    "    assert lengths == original_lengths, \"Samples don't match\"\n",
    "    for batch in loader:\n",
    "        assert len(torch.unique(batch[1])) == 1, \"Non unique length in the batch\"\n",
    "        assert len(batch[0]) == len(batch[1])\n",
    "    print(\"Verification successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/tmp.RIDayGxO54/ipykernel_2493705/606805386.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sampler = LengthBasedSampler(torch.load(lengths_path), batch_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification successful!\n",
      "Verification successful!\n",
      "Verification successful!\n"
     ]
    }
   ],
   "source": [
    "verify(lengths_path=\"protein_lengths_train.pt\", batch_size=32, ds=train_ds, df=filtered_train_ds)\n",
    "verify(lengths_path=\"protein_lengths_val.pt\", batch_size=32, ds=val_ds, df=filtered_val_ds)\n",
    "verify(lengths_path=\"protein_lengths_test.pt\", batch_size=32, ds=test_ds, df=filtered_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/tmp.RIDayGxO54/ipykernel_2493705/3445233032.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sampler = LengthBasedSampler(torch.load(\"protein_lengths_train.pt\"), 32)\n"
     ]
    }
   ],
   "source": [
    "sampler = LengthBasedSampler(torch.load(\"protein_lengths_train.pt\"), 32)\n",
    "loader = DataLoader(train_ds, batch_sampler=sampler)\n",
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, lengths = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 6])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
