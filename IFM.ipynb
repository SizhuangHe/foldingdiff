{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh2748/.conda/envs/foldingdiff/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import argparse\n",
    "import functools\n",
    "from datetime import datetime\n",
    "from typing import *\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from torch.distributions import Normal, kl_divergence\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.strategies.ddp import DDPStrategy\n",
    "\n",
    "from transformers import BertConfig\n",
    "\n",
    "from foldingdiff import datasets\n",
    "from foldingdiff import modelling\n",
    "from foldingdiff import losses\n",
    "from foldingdiff import beta_schedules\n",
    "from foldingdiff import plotting\n",
    "from foldingdiff import utils\n",
    "from foldingdiff import custom_metrics as cm\n",
    "\n",
    "assert torch.cuda.is_available(), \"Requires CUDA to train\"\n",
    "# reproducibility\n",
    "torch.manual_seed(6489)\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define some typing literals\n",
    "ANGLES_DEFINITIONS = Literal[\n",
    "    \"canonical\", \"canonical-full-angles\", \"canonical-minimal-angles\", \"cart-coords\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulo_with_wrapped_range(\n",
    "    vals, range_min: float = -np.pi, range_max: float = np.pi\n",
    "):\n",
    "    \"\"\"\n",
    "    Modulo with wrapped range -- capable of handing a range with a negative min\n",
    "\n",
    "    modulo_with_wrapped_range(3, -2, 2)\n",
    "    -1\n",
    "    \"\"\"\n",
    "    assert range_min <= 0.0\n",
    "    assert range_min < range_max\n",
    "\n",
    "    # Modulo after we shift values\n",
    "    top_end = range_max - range_min\n",
    "    # Shift the values to be in the range [0, top_end)\n",
    "    vals_shifted = vals - range_min\n",
    "    # Perform modulo\n",
    "    vals_shifted_mod = vals_shifted % top_end\n",
    "    # Shift back down\n",
    "    retval = vals_shifted_mod + range_min\n",
    "\n",
    "    # Checks\n",
    "    # print(\"Mod return\", vals, \" --> \", retval)\n",
    "    # if isinstance(retval, torch.Tensor):\n",
    "    #     notnan_idx = ~torch.isnan(retval)\n",
    "    #     assert torch.all(retval[notnan_idx] >= range_min)\n",
    "    #     assert torch.all(retval[notnan_idx] < range_max)\n",
    "    # else:\n",
    "    #     assert (\n",
    "    #         np.nanmin(retval) >= range_min\n",
    "    #     ), f\"Illegal value: {np.nanmin(retval)} < {range_min}\"\n",
    "    #     assert (\n",
    "    #         np.nanmax(retval) <= range_max\n",
    "    #     ), f\"Illegal value: {np.nanmax(retval)} > {range_max}\"\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a PyTorch Dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe=None, data_path=None):\n",
    "        if data_path is not None:\n",
    "            print(f\"Initializing from a data path: {data_path}\")\n",
    "            self.data = torch.load(data_path)\n",
    "        elif data_path is None and dataframe is not None:\n",
    "            print(\"Initializing from a dataframe.\")\n",
    "            self.data = torch.stack(dataframe['angles'].tolist())\n",
    "        else:\n",
    "            raise Exception(\"Dataframe and data path are both None!\")\n",
    "        \n",
    "         \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from a data path: protein_angles_100sanity_128.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/tmp.iD1eevmCgx/ipykernel_1484158/2395865528.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data = torch.load(data_path)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dataset with the DataFrame\n",
    "dataset = MyDataset(data_path=\"protein_angles_100sanity_128.pt\")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,       # Number of samples per batch\n",
    "    shuffle=True,        # Shuffle the data at every epoch\n",
    "    num_workers=1        # Number of subprocesses to use for data loading\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise is not varying.\n"
     ]
    }
   ],
   "source": [
    "from IntegralFlowMatching.integral_flow_matching import IntegralFlowMatcher\n",
    "IFM = IntegralFlowMatcher(sigma=0.1, same_time=True, time_interp=True, noise_on_x0=True, noise_on_x1=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))\n",
    "x0 = torch.randn_like(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:torch.Size([8, 8, 128, 6]), torch.Size([8, 8, 128, 6])\n",
      "Any out of range values in xt: True\n",
      "Any out of range values in mu_t: True\n",
      "After transformation:\n",
      "Shapes:torch.Size([8, 8, 128, 6]), torch.Size([8, 8, 128, 6])\n",
      "Any out of range values in xt: False\n",
      "Any out of range values in mu_t: False\n"
     ]
    }
   ],
   "source": [
    "t, xt, mu_t = IFM.sample_conditional_flow(rearrange(x0, 'b s d -> b (s d)').to(\"cuda\"), rearrange(batch, 'b s d -> b (s d)').to(\"cuda\"), 8,  device = \"cuda\")\n",
    "xt = rearrange(xt, 'b t (s d) -> b t s d', s=x0.shape[1])\n",
    "mu_t = rearrange(mu_t, 'b t (s d) -> b t s d', s=x0.shape[1])\n",
    "\n",
    "print(f\"Shapes:{xt.shape}, {mu_t.shape}\")\n",
    "print(f\"Any out of range values in xt: {((xt < -np.pi) | (xt > np.pi)).any().item()}\")\n",
    "print(f\"Any out of range values in mu_t: {((mu_t < -np.pi) | (mu_t > np.pi)).any().item()}\")\n",
    "xt = modulo_with_wrapped_range(xt)\n",
    "mu_t = modulo_with_wrapped_range(mu_t)\n",
    "print(\"After transformation:\")\n",
    "print(f\"Shapes:{xt.shape}, {mu_t.shape}\")\n",
    "print(f\"Any out of range values in xt: {((xt < -np.pi) | (xt > np.pi)).any().item()}\")\n",
    "print(f\"Any out of range values in mu_t: {((mu_t < -np.pi) | (mu_t > np.pi)).any().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob=0.1):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "from ipdb import set_trace\n",
    "\n",
    "# Copied from DanielFLevine/ifm/utils/modules.py\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_prob=0.1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.norm = nn.LayerNorm(output_dim, elementwise_affine=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.linear(x)\n",
    "        out = self.norm(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        # out += identity  # Skip connection\n",
    "        return out\n",
    "\n",
    "class MLPLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_prob=0.1):\n",
    "        super(MLPLayer, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.norm = nn.LayerNorm(output_dim, elementwise_affine=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.linear(x)\n",
    "        out = self.norm(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class MidFC(nn.Module):\n",
    "    def __init__(self, dim, num_layers, dropout_prob=0.1):\n",
    "        super(MidFC, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            *[MLPLayer(dim, dim) for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class CustomDecoder(nn.Module):\n",
    "    def __init__(self,input_dim, hidden_dim, output_dim, num_blocks=1):\n",
    "        super(CustomDecoder, self).__init__()\n",
    "        self.initial_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim, elementwise_affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1)\n",
    "        )\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(input_dim=hidden_dim, output_dim=hidden_dim) for _ in range(num_blocks)]\n",
    "        )\n",
    "        self.final_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_layer(x)\n",
    "        x = self.residual_blocks(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x\n",
    "\n",
    "class CustomVAEDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, vae_latent_dim, decoder_hidden_dim, output_dim, num_blocks=1):\n",
    "        super(CustomVAEDecoder, self).__init__()\n",
    "        self.mean_encoder = ResidualBlock(input_dim=input_dim, output_dim=vae_latent_dim)\n",
    "        self.var_encoder = ResidualBlock(input_dim=input_dim, output_dim=vae_latent_dim)\n",
    "        self.var_activation = torch.exp\n",
    "        self.var_eps = 0.0001\n",
    "        self.decoder = CustomDecoder(\n",
    "            input_dim=vae_latent_dim,\n",
    "            hidden_dim=decoder_hidden_dim,\n",
    "            output_dim=output_dim,\n",
    "            num_blocks=num_blocks\n",
    "        )\n",
    "\n",
    "    def forward(self, x, temperature=1.0):\n",
    "        '''\n",
    "            Returns:\n",
    "                outputs:    output representations\n",
    "                latents:    VAE latent representations\n",
    "                dist:       a torch.dist object used to compute KL divergence\n",
    "        '''\n",
    "        # set_trace()\n",
    "        mu = self.mean_encoder(x)\n",
    "        var = self.var_encoder(x)\n",
    "        var = self.var_activation(var) + self.var_eps\n",
    "        dist = Normal(mu, temperature*(var.sqrt()))\n",
    "        latents = dist.rsample()\n",
    "        outputs = self.decoder(latents)\n",
    "        # set_trace()\n",
    "        return outputs, latents, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Config\n",
    "class ProteinAngleFlowModel(nn.Module):\n",
    "    def __init__(self, \n",
    "        input_size, \n",
    "        proj_hid_size,\n",
    "        llm_embd_size,\n",
    "        num_res_per_group=1,\n",
    "        llm_name=\"gpt2\",\n",
    "        use_pretrained_weights=True,\n",
    "        use_custom_gpt2_arch=False,\n",
    "        llm_n_layer=1,\n",
    "        llm_n_head=1,\n",
    "        vae_latent_dim=128,\n",
    "        decoder_hidden_dim=256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_res_per_group = num_res_per_group\n",
    "        self.proj_in = SimpleMLP(input_size=input_size, hidden_size=proj_hid_size, output_size=int(llm_embd_size/num_res_per_group))\n",
    "        self._create_llm(llm_name, use_custom_gpt2_arch, use_pretrained_weights, llm_embd_size, llm_n_layer, llm_n_head)\n",
    "        self.vae_decoder = CustomVAEDecoder(input_dim=llm_embd_size, vae_latent_dim=vae_latent_dim, decoder_hidden_dim=decoder_hidden_dim, output_dim=input_size*num_res_per_group)\n",
    "    \n",
    "    def _create_llm(self, llm_name, use_custom_gpt2_arch, use_pretrained_weights, llm_embd_size, llm_n_layer, llm_n_head):\n",
    "        if llm_name == \"gpt2\": \n",
    "            print(\"Use GPT2 as LLM.\")\n",
    "            if not use_custom_gpt2_arch:\n",
    "                print(\"Use GPT2 model with pretrained architecture!\")\n",
    "                if llm_embd_size != 768:\n",
    "                    print(f\"Override GPT2 dimension from {self._gpt_conf.n_embd} to 768!\")\n",
    "                if use_pretrained_weights:\n",
    "                    self.llm_model = GPT2Model.from_pretrained('gpt2') \n",
    "                    print(\"Use GPT2 model with PRETRAINED weights!\")\n",
    "                else:\n",
    "                    config = GPT2Config()  # Default GPT-2 configuration\n",
    "                    self.llm_model = GPT2Model(config=config)\n",
    "                    print(\"Use GPT2 model with RANDOM weights!\")\n",
    "            else:\n",
    "                config = GPT2Config(\n",
    "                    n_embd=llm_embd_size,\n",
    "                    n_layer=llm_n_layer,\n",
    "                    n_head=llm_n_head\n",
    "                )\n",
    "                self.llm_model = GPT2Model(config=config)\n",
    "                print(\"Use CUSTOM GPT2 architecture!\")\n",
    "                print(\"Use GPT2 model with RANDOM weights!\")\n",
    "            self.llm_model.wte = None\n",
    "        elif self.llm_name in [\"pythia-160m\", 'pythia-410m']:\n",
    "            llm_name = 'EleutherAI/' + self._model_conf.llm_name\n",
    "            print(f\"Use {llm_name} as LLM.\")\n",
    "            if use_pretrained_weights:\n",
    "                print(f\"Use {llm_name} model with PRETRAINED weights!\")\n",
    "                self.llm_model = AutoModel.from_pretrained(llm_name)\n",
    "            else:\n",
    "                pythia_config = AutoConfig.from_pretrained(llm_name)\n",
    "                self.llm_model = AutoModel.from_config(pythia_config)\n",
    "                print(f\"Use {llm_name} model with RANDOM weights!\")\n",
    "\n",
    "    def forward(self, protein_angles):\n",
    "        residue_embeds = self.proj_in(protein_angles) # [b, t, s, d]\n",
    "        _b, _t, _s, _d = residue_embeds.shape\n",
    "        group_embeds = rearrange(residue_embeds, 'b t (s0 g) d -> b (t s0) (g d)', g = self.num_res_per_group) # [b, t*s/g, g*d], g: num_res_per_group\n",
    "        llm_outputs = self.llm_model(inputs_embeds=group_embeds)\n",
    "        llm_outputs_embeds = llm_outputs.last_hidden_state\n",
    "        vae_outputs, vae_latents, vae_dist = self.vae_decoder(llm_outputs_embeds)\n",
    "        outputs_embeds = rearrange(vae_outputs, 'b (t s0) (g d) -> b t (s0 g) d', s0=int(_s/self.num_res_per_group),g = self.num_res_per_group) # [b, t, s, d], g: num_res_per_group\n",
    "        outputs_embeds = modulo_with_wrapped_range(outputs_embeds)\n",
    "        # KL divergence loss\n",
    "        z_cond_dist = vae_dist\n",
    "        z_prior_dist = Normal(torch.zeros_like(vae_latents), torch.ones_like(vae_latents))\n",
    "        # set_trace()\n",
    "        kl_divergence_z = kl_divergence(\n",
    "                        z_cond_dist,\n",
    "                        z_prior_dist\n",
    "                    )\n",
    "        kl_divergence_z = rearrange(kl_divergence_z, 'b (t s0) (g d) -> b t (s0 g) d', s0=int(_s/self.num_res_per_group), g=self.num_res_per_group) #[b, t, s, d]\n",
    "        kl_divergence_z = kl_divergence_z.sum(dim=-1)[:,:-1,:] # throw away the last time point\n",
    "        return {\n",
    "            'output_embeds': outputs_embeds,\n",
    "            'kl_divergence': kl_divergence_z.mean()\n",
    "        }\n",
    "    \n",
    "    def generate_next_tokens(self, protein_angles, temperature=1):\n",
    "        assert not temperature < 0, \"Temperature should be non-negative!\"\n",
    "        residue_embeds = self.proj_in(protein_angles) # [b, t, s, d]\n",
    "        _b, _t, _s, _d = residue_embeds.shape\n",
    "        group_embeds = rearrange(residue_embeds, 'b t (s0 g) d -> b (t s0) (g d)', g = self.num_res_per_group) # [b, t*s/g, g*d], g: num_res_per_group\n",
    "        llm_outputs = self.llm_model(inputs_embeds=group_embeds)\n",
    "        llm_outputs_embeds = llm_outputs.last_hidden_state\n",
    "        vae_outputs, vae_latents, vae_dist = self.vae_decoder(llm_outputs_embeds, temperature=temperature)\n",
    "        outputs_embeds = rearrange(vae_outputs, 'b (t s0) (g d) -> b t (s0 g) d', s0=int(_s/self.num_res_per_group),g = self.num_res_per_group) # [b, t, s, d], g: num_res_per_group\n",
    "        outputs_embeds = modulo_with_wrapped_range(outputs_embeds)\n",
    "        \n",
    "        return outputs_embeds[:,-1,:,:].unsqueeze(1)\n",
    "\n",
    "    def generate(self, input_ids, temperature=1, max_length=8):\n",
    "        output_sequences = input_ids\n",
    "        while output_sequences.shape[1] < max_length:\n",
    "            # print(torch.cuda.memory_allocated())\n",
    "            next_tokens = self.generate_next_tokens(output_sequences)\n",
    "            output_sequences = torch.cat([output_sequences, next_tokens], dim=1)\n",
    "        return output_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPT2 as LLM.\n",
      "Use GPT2 model with pretrained architecture!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh2748/.conda/envs/foldingdiff/lib/python3.8/site-packages/transformers/modeling_utils.py:1331: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPT2 model with PRETRAINED weights!\n"
     ]
    }
   ],
   "source": [
    "protein_angle_model = ProteinAngleFlowModel(input_size=6, proj_hid_size=128, llm_embd_size=768).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = x0.unsqueeze(1).to(torch.float).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358359552\n",
      "41170448896\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "aaa = protein_angle_model.generate(x0)\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391913984\n"
     ]
    }
   ],
   "source": [
    "del aaa\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391913984\n",
      "41171235328\n",
      "391913984\n"
     ]
    }
   ],
   "source": [
    "bbb=[]\n",
    "print(torch.cuda.memory_allocated())\n",
    "aaa = protein_angle_model.generate(x0)\n",
    "# aaa_cpu = aaa.to(\"cpu\")\n",
    "# bbb.append(aaa_cpu)\n",
    "print(torch.cuda.memory_allocated())\n",
    "del aaa\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391913984\n",
      "391913984\n",
      "391913984\n",
      "391913984\n"
     ]
    }
   ],
   "source": [
    "bbb=[]\n",
    "print(torch.cuda.memory_allocated())\n",
    "aaa = protein_angle_model.generate(x0).cpu().detach().numpy()\n",
    "print(torch.cuda.memory_allocated())\n",
    "# aaa_cpu = aaa.clone().to(\"cpu\")\n",
    "# print(torch.cuda.memory_allocated())\n",
    "torch.cuda.empty_cache()\n",
    "# print(torch.cuda.memory_allocated())\n",
    "bbb.append(aaa)\n",
    "# print(torch.cuda.memory_allocated())\n",
    "del aaa\n",
    "\n",
    "print(torch.cuda.memory_allocated())\n",
    "# del aaa_cpu\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391913984\n"
     ]
    }
   ],
   "source": [
    "del bbb[0]\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8, 128, 6])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "bbb = []\n",
    "for i in tqdm(range(10)):\n",
    "    # print(torch.cuda.memory_allocated())\n",
    "    aaa = protein_angle_model.generate(x0)[:,-1,:,:].cpu().detach()\n",
    "    bbb.append(aaa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 128, 6])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "torch.cat(bbb, dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sequences = x0.unsqueeze(1).to(torch.float).to(\"cuda\")\n",
    "while output_sequences.shape[1] < max_length:\n",
    "    # print(output_sequences.shape[1])\n",
    "    next_tokens = self.generate_next_tokens(output_sequences)\n",
    "    # print(output_sequences.shape)\n",
    "    # print(next_tokens.shape)\n",
    "    output_sequences = torch.cat([output_sequences, next_tokens], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = protein_angle_model(xt.to(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00001\n",
    "weight_decay = 0\n",
    "beta = 0.3\n",
    "num_epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(protein_angle_model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "protein_angle_model = protein_angle_model.to(\"cuda\")\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = batch.to(\"cuda\")\n",
    "        x0 = torch.randn_like(batch)\n",
    "        t, xt, mu_t = IFM.sample_conditional_flow(rearrange(x0, 'b s d -> b (s d)').to(\"cuda\"), rearrange(batch, 'b s d -> b (s d)').to(\"cuda\"), 8,  device = \"cuda\")\n",
    "        xt = rearrange(xt, 'b t (s d) -> b t s d', s=x0.shape[1])\n",
    "        xt = modulo_with_wrapped_range(xt)\n",
    "\n",
    "\n",
    "        xt = xt.to(torch.float32)\n",
    "    \n",
    "        outputs = protein_angle_model(xt)\n",
    "        shifted_pred_tokens = outputs[\"output_embeds\"][:, :-1, ...]\n",
    "        shifted_gt_tokens = xt[:,1:,...]\n",
    "\n",
    "        mse_loss = F.mse_loss(shifted_pred_tokens, shifted_gt_tokens, reduction=\"none\").sum(dim=-1).mean()\n",
    "        kl_div = outputs['kl_divergence']\n",
    "        loss = mse_loss + beta * kl_div\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "       \n",
    "    print(\"Epoch {}: loss: {}\".format(epoch+1, epoch_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
